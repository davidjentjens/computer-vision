{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01f6154f",
   "metadata": {},
   "source": [
    "## Rede Neural - MNIST\n",
    "#### Aluno: David Maisonnette Jentjens - 1810235\n",
    "\n",
    "Este notebook contém uma implementação de um algoritmo de deep learning, capaz de classificar imagens de digitos da biblioteca MNIST, através de um aprendizado baseado na técnica de gradiente descendente estocástico. O modelo é implementado apenas utilizando-se o numpy e algumas outras para funcionalidades menores, que não impactam no aprendizado da modelo em si. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee821a5",
   "metadata": {},
   "source": [
    "### 0 - Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c9d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "    \n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00d4ea",
   "metadata": {},
   "source": [
    "### 1 - Preprocessamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a55473",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "x = (x/255).astype('float32')\n",
    "y = to_categorical(y)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c924bc7",
   "metadata": {},
   "source": [
    "### 2 - Funções de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f06ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    \n",
    "def softmax(x, derivative=False):\n",
    "    exps = np.exp(x - x.max())\n",
    "\n",
    "    if derivative:\n",
    "        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "\n",
    "    return exps / np.sum(exps, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6a0ad74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learn_rate=0.001, epochs=10):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learn_rate = learn_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.params = self.initialization()\n",
    "    \n",
    "    def initialization(self):\n",
    "        input_layer = self.layer_sizes[0]\n",
    "        hidden_layer_1 = self.layer_sizes[1]\n",
    "        hidden_layer_2 = self.layer_sizes[2]\n",
    "        output_layer = self.layer_sizes[3]\n",
    "\n",
    "        params = {\n",
    "            'W1':np.random.randn(hidden_layer_1, input_layer) * np.sqrt(1. / hidden_layer_1),\n",
    "            'W2':np.random.randn(hidden_layer_2, hidden_layer_1) * np.sqrt(1. / hidden_layer_2),\n",
    "            'W3':np.random.randn(output_layer, hidden_layer_2) * np.sqrt(1. / output_layer)\n",
    "        }\n",
    "\n",
    "        return params\n",
    "\n",
    "    def propagate_forward(self, x_train):\n",
    "        params = self.params\n",
    "\n",
    "        if(x_train[0] == 'p'):\n",
    "            return\n",
    "        \n",
    "        # input layer activations becomes sample\n",
    "        params['A0'] = x_train\n",
    "\n",
    "        # input layer to hidden layer 1\n",
    "        params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
    "        params['A1'] = sigmoid(params['Z1'])\n",
    "\n",
    "        # hidden layer 1 to hidden layer 2\n",
    "        params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
    "        params['A2'] = sigmoid(params['Z2'])\n",
    "\n",
    "        # hidden layer 2 to output layer\n",
    "        params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
    "        params['A3'] = softmax(params['Z3'])\n",
    "\n",
    "        return params['A3']\n",
    "\n",
    "    def propagate_backward(self, y_train, output):\n",
    "        params = self.params\n",
    "        change_w = {}\n",
    "\n",
    "        # Calculate W3 update\n",
    "        error = 2 * (output - y_train) / output.shape[0] * softmax(params['Z3'], derivative=True)\n",
    "        change_w['W3'] = np.outer(error, params['A2'])\n",
    "\n",
    "        # Calculate W2 update\n",
    "        error = np.dot(params['W3'].T, error) * sigmoid(params['Z2'], derivative=True)\n",
    "        change_w['W2'] = np.outer(error, params['A1'])\n",
    "\n",
    "        # Calculate W1 update\n",
    "        error = np.dot(params['W2'].T, error) * sigmoid(params['Z1'], derivative=True)\n",
    "        change_w['W1'] = np.outer(error, params['A0'])\n",
    "\n",
    "        return change_w\n",
    "\n",
    "    def update_network_parameters(self, changes_to_w):\n",
    "        for key, value in changes_to_w.items():\n",
    "            self.params[key] -= self.learn_rate * value\n",
    "\n",
    "    def fit(self, x_train, y_train, x_val, y_val):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for iteration in range(self.epochs):\n",
    "            for (i, x), y in zip(x_train.iterrows(), y_train):\n",
    "                output = self.propagate_forward(x)\n",
    "                changes_to_w = self.propagate_backward(y, output)\n",
    "                self.update_network_parameters(changes_to_w)\n",
    "\n",
    "            accuracy = self.calc_accuracy(x_val, y_val)\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
    "                iteration+1, time.time() - start_time, accuracy * 100\n",
    "            ))\n",
    "\n",
    "    def calc_accuracy(self, x_val, y_val):\n",
    "        predictions = []\n",
    "\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.propagate_forward(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(y))\n",
    "\n",
    "        return np.mean(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2e306578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Time Spent: 38.71s, Accuracy: 9.82%\n",
      "Epoch: 2, Time Spent: 73.07s, Accuracy: 9.82%\n",
      "Epoch: 3, Time Spent: 107.68s, Accuracy: 9.82%\n",
      "Epoch: 4, Time Spent: 139.99s, Accuracy: 9.82%\n",
      "Epoch: 5, Time Spent: 173.36s, Accuracy: 9.82%\n",
      "Epoch: 6, Time Spent: 207.00s, Accuracy: 9.82%\n",
      "Epoch: 7, Time Spent: 239.53s, Accuracy: 9.82%\n",
      "Epoch: 8, Time Spent: 273.22s, Accuracy: 9.82%\n",
      "Epoch: 9, Time Spent: 309.75s, Accuracy: 9.82%\n",
      "Epoch: 10, Time Spent: 347.52s, Accuracy: 9.82%\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(layer_sizes=[784, 128, 64, 10])\n",
    "model.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145eeb20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
